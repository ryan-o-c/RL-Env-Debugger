{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f2b1ac-50d6-42d7-960c-b970912a4abb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RL Import\n",
      "Are we getting this far?\n",
      "here?\n",
      "Float32[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# MyGrid\n",
       "\n",
       "## Traits\n",
       "\n",
       "| Trait Type        |                          Value |\n",
       "|:----------------- | ------------------------------:|\n",
       "| NumAgentStyle     |                  SingleAgent() |\n",
       "| DynamicStyle      |                   Sequential() |\n",
       "| InformationStyle  |           PerfectInformation() |\n",
       "| ChanceStyle       |                Deterministic() |\n",
       "| RewardStyle       |                   StepReward() |\n",
       "| UtilityStyle      |                   GeneralSum() |\n",
       "| ActionStyle       |             MinimalActionSet() |\n",
       "| StateStyle        | Observation{Vector{Float32}}() |\n",
       "| DefaultStateStyle | Observation{Vector{Float32}}() |\n",
       "\n",
       "## Is Environment Terminated?\n",
       "\n",
       "No\n",
       "\n",
       "## State Space\n",
       "\n",
       "`Space{Vector{Interval{Int64, Closed, Closed}}}(Interval{Int64, Closed, Closed}[Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50)])`\n",
       "\n",
       "## Action Space\n",
       "\n",
       "`Base.OneTo(25)`\n",
       "\n",
       "## Current State\n",
       "\n",
       "```\n",
       "Float32[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "```\n"
      ],
      "text/plain": [
       "# MyGrid\n",
       "\n",
       "## Traits\n",
       "\n",
       "| Trait Type        |                          Value |\n",
       "|:----------------- | ------------------------------:|\n",
       "| NumAgentStyle     |                  SingleAgent() |\n",
       "| DynamicStyle      |                   Sequential() |\n",
       "| InformationStyle  |           PerfectInformation() |\n",
       "| ChanceStyle       |                Deterministic() |\n",
       "| RewardStyle       |                   StepReward() |\n",
       "| UtilityStyle      |                   GeneralSum() |\n",
       "| ActionStyle       |             MinimalActionSet() |\n",
       "| StateStyle        | Observation{Vector{Float32}}() |\n",
       "| DefaultStateStyle | Observation{Vector{Float32}}() |\n",
       "\n",
       "## Is Environment Terminated?\n",
       "\n",
       "No\n",
       "\n",
       "## State Space\n",
       "\n",
       "`Space{Vector{Interval{Int64, Closed, Closed}}}(Interval{Int64, Closed, Closed}[Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50)])`\n",
       "\n",
       "## Action Space\n",
       "\n",
       "`Base.OneTo(25)`\n",
       "\n",
       "## Current State\n",
       "\n",
       "```\n",
       "Float32[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "```\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ReinforcementLearning, Intervals, Flux\n",
    "\n",
    "const RLBase = ReinforcementLearningBase\n",
    "\n",
    "#Need to make it so I use include(\"some.jl\") instead but for now\n",
    "#using NBInclude\n",
    "#@nbinclude(\"RIS_Solvers.ipynb\")\n",
    "\n",
    "println(\"Done RL Import\")\n",
    "\n",
    "Base.@kwdef mutable struct MyGrid <: AbstractEnv\n",
    "    N::Int\n",
    "    #We start with squares start spread across the line at co-ordinates 1,2,3,4,5,6,7\n",
    "    obs::Vector{Float32}\n",
    "    max_steps::Int    \n",
    "    \n",
    "    rewards::Float32\n",
    "    done::Bool \n",
    "    t::Int\n",
    "end\n",
    "\n",
    "function MyGrid(; n)\n",
    "    println(\"here?\")\n",
    "    img = zeros(Float32, n,n)\n",
    "    vect = ones(Float32, 3)\n",
    "    obs = vcat(vect,vec(img))\n",
    "    println(obs)\n",
    "    \n",
    "\tMyGrid(n, obs, 10, 0, false, 0)\n",
    "end\n",
    "\n",
    "RLBase.action_space(env::MyGrid) = Base.OneTo(25)\n",
    "\n",
    "function (env::MyGrid)(action) #This function takes the action, makes the environment step, and gives reward\n",
    "    \n",
    "    env.obs[action[1]] = 6\n",
    "    \n",
    "    env.rewards+=1\n",
    "    \n",
    "    env.t+=1\n",
    "    #Max steps check\n",
    "    if env.t >= env.max_steps\n",
    "        env.done = true\n",
    "    end\n",
    "end\n",
    "\n",
    "RLBase.state(env::MyGrid, ::Observation{Vector{Float32}}) = env.obs\n",
    "RLBase.state_space(env::MyGrid, ::Observation{Vector{Float32}}) = Space(fill( Interval{Int64}(0, (10*env.N) ) , (env.N^2+3)))\n",
    "\n",
    "RLBase.is_terminated(env::MyGrid) = env.done\n",
    "\n",
    "function RLBase.reset!(env::MyGrid)\n",
    "    #Reset Counter and rewards (and termination variable)\n",
    "    env.t = 0\n",
    "    env.rewards = 0\n",
    "    env.done = false    \n",
    "    \n",
    "    img = zeros(env.N,env.N)\n",
    "    vect = ones(3)\n",
    "    env.obs = vcat(vect,vec(img))\n",
    "end;\n",
    "\n",
    "RLBase.reward(env::MyGrid) = env.rewards\n",
    "\n",
    "RLBase.NumAgentStyle(::MyGrid) = SINGLE_AGENT\n",
    "RLBase.DynamicStyle(::MyGrid) = SEQUENTIAL\n",
    "RLBase.ActionStyle(::MyGrid) = MINIMAL_ACTION_SET\n",
    "RLBase.InformationStyle(::MyGrid) = PERFECT_INFORMATION\n",
    "\n",
    "RLBase.StateStyle(::MyGrid) = Observation{Vector{Float32}}()\n",
    "\n",
    "RLBase.RewardStyle(::MyGrid) = STEP_REWARD\n",
    "RLBase.UtilityStyle(::MyGrid) = GENERAL_SUM\n",
    "RLBase.ChanceStyle(::MyGrid) = DETERMINISTIC\n",
    "\n",
    "println(\"Are we getting this far?\")\n",
    "N=5\n",
    "env = MyGrid(; n=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "877e5d06-fd7d-49e6-954a-8d19e43649e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdfghjkl;\n"
     ]
    }
   ],
   "source": [
    "println(\"asdfghjkl;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11afa85d-aeab-40db-80d9-d5a3a2906e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length(state_space(env)) == length(env.obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b7c0a9f-6e74-4076-a31f-9cf576940d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TotalBatchRewardPerEpisode([Float64[]], [0.0], true)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UPDATE_FREQ = 32\n",
    "N_ENV = 1\n",
    "\n",
    "agent = Agent(\n",
    "        policy = PPOPolicy(\n",
    "            approximator = ActorCritic(\n",
    "                    actor = Chain(\n",
    "                                x -> ( reshape(x[4:28], (5,5,1,1)), x[1:3]),\n",
    "                                Parallel(vcat, \n",
    "                                        Chain(Conv((3,3), 1 => 1, relu; stride = 1, pad = 0),\n",
    "                                        Flux.flatten),\n",
    "\n",
    "                                        Chain(Dense(3, 6, relu), Dense(6, 3, relu), Dense(3, 1, relu)) ),\n",
    "                                Dense(10, 20, relu),\n",
    "                                Dense(20,5,relu),\n",
    "                                Dense(5,3)),\n",
    "\n",
    "                    critic = Chain(\n",
    "                                x -> ( reshape(x[4:28], (5,5,1,1)), x[1:3]),\n",
    "                                Parallel(vcat, \n",
    "                                        Chain(Conv((3,3), 1 => 1, relu; stride = 1, pad = 0),\n",
    "                                        Flux.flatten),\n",
    "\n",
    "                                        Chain(Dense(3, 6, relu), Dense(6, 3, relu), Dense(3, 1, relu)) ),\n",
    "                                Dense(10, 20, relu),\n",
    "                                Dense(20,5,relu),\n",
    "                                Dense(5,1)),\n",
    "            \n",
    "                optimizer = ADAM(1e-3),\n",
    "            ),\n",
    "            γ = 0.99f0,\n",
    "            λ = 0.95f0,\n",
    "            clip_range = 0.1f0,\n",
    "            max_grad_norm = 0.5f0,\n",
    "            n_epochs = 4,\n",
    "            n_microbatches = 4,\n",
    "            actor_loss_weight = 1.0f0,\n",
    "            critic_loss_weight = 0.5f0,\n",
    "            entropy_loss_weight = 0.001f0,\n",
    "            update_freq = UPDATE_FREQ,\n",
    "        ),\n",
    "        trajectory = PPOTrajectory(;\n",
    "            capacity = UPDATE_FREQ, \n",
    "            state = Matrix{Float32} => (28, N_ENV),\n",
    "            action = Vector{Int} => (N_ENV,),\n",
    "            action_log_prob = Vector{Float32} => (N_ENV,),\n",
    "            reward = Vector{Float32} => (N_ENV,),  \n",
    "            terminal = Vector{Bool} => (N_ENV,),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "#stop_condition = StopAfterStep(30000, is_show_progress=true)\n",
    "hook = TotalBatchRewardPerEpisode(N_ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45a24388-252d-4358-b108-3e783c343514",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  25%|██████████▎                              |  ETA: 0:00:51\u001b[39m"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "BoundsError: attempt to access 1-element view(::Matrix{Float32}, 1, :) with eltype Float32 at index [33]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 1-element view(::Matrix{Float32}, 1, :) with eltype Float32 at index [33]",
      "",
      "Stacktrace:",
      "  [1] throw_boundserror(A::SubArray{Float32, 1, Matrix{Float32}, Tuple{Int64, Base.Slice{Base.OneTo{Int64}}}, true}, I::Tuple{Int64})",
      "    @ Base ./abstractarray.jl:691",
      "  [2] checkbounds",
      "    @ ./abstractarray.jl:656 [inlined]",
      "  [3] getindex",
      "    @ ./subarray.jl:302 [inlined]",
      "  [4] _generalized_advantage_estimation!(advantages::SubArray{Float32, 1, Matrix{Float32}, Tuple{Int64, Base.Slice{Base.OneTo{Int64}}}, true}, rewards::SubArray{Float32, 1, Matrix{Float32}, Tuple{Int64, Vector{Int64}}, false}, values::SubArray{Float32, 1, Matrix{Float32}, Tuple{Int64, Base.Slice{Base.OneTo{Int64}}}, true}, γ::Float32, λ::Float32, terminal::SubArray{Bool, 1, Matrix{Bool}, Tuple{Int64, Vector{Int64}}, false})",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/utils/base.jl:445",
      "  [5] _generalized_advantage_estimation!(advantages::Matrix{Float32}, rewards::CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, values::Matrix{Float32}, γ::Float32, λ::Float32, terminal::CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}, dims::Int64)",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/utils/base.jl:425",
      "  [6] #generalized_advantage_estimation!#24",
      "    @ ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/utils/base.jl:387 [inlined]",
      "  [7] #generalized_advantage_estimation#23",
      "    @ ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/utils/base.jl:375 [inlined]",
      "  [8] _update!(p::PPOPolicy{ActorCritic{Chain{Tuple{var\"#3#5\", Parallel{typeof(vcat), Tuple{Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, typeof(flatten)}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{var\"#4#6\", Parallel{typeof(vcat), Tuple{Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, typeof(flatten)}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Distributions.Categorical{P} where P<:Real, Random._GLOBAL_RNG}, t::Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularArrayBuffer{Float32, 3, Array{Float32, 3}}, CircularArrayBuffers.CircularArrayBuffer{Int64, 2, Matrix{Int64}}, CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}})",
      "    @ ReinforcementLearningZoo ~/.julia/packages/ReinforcementLearningZoo/uNyVA/src/algorithms/policy_gradient/ppo.jl:244",
      "  [9] update!",
      "    @ ~/.julia/packages/ReinforcementLearningZoo/uNyVA/src/algorithms/policy_gradient/ppo.jl:210 [inlined]",
      " [10] (::Agent{PPOPolicy{ActorCritic{Chain{Tuple{var\"#3#5\", Parallel{typeof(vcat), Tuple{Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, typeof(flatten)}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{var\"#4#6\", Parallel{typeof(vcat), Tuple{Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, typeof(flatten)}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Distributions.Categorical{P} where P<:Real, Random._GLOBAL_RNG}, Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularArrayBuffer{Float32, 3, Array{Float32, 3}}, CircularArrayBuffers.CircularArrayBuffer{Int64, 2, Matrix{Int64}}, CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}}})(stage::PreActStage, env::MyGrid, action::Vector{Int64})",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/policies/agents/agent.jl:78",
      " [11] _run(policy::Agent{PPOPolicy{ActorCritic{Chain{Tuple{var\"#3#5\", Parallel{typeof(vcat), Tuple{Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, typeof(flatten)}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{var\"#4#6\", Parallel{typeof(vcat), Tuple{Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, typeof(flatten)}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Distributions.Categorical{P} where P<:Real, Random._GLOBAL_RNG}, Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularArrayBuffer{Float32, 3, Array{Float32, 3}}, CircularArrayBuffers.CircularArrayBuffer{Int64, 2, Matrix{Int64}}, CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}}}, env::MyGrid, stop_condition::StopAfterEpisode{ProgressMeter.Progress}, hook::TotalBatchRewardPerEpisode)",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/core/run.jl:29",
      " [12] run(policy::Agent{PPOPolicy{ActorCritic{Chain{Tuple{var\"#3#5\", Parallel{typeof(vcat), Tuple{Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, typeof(flatten)}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{var\"#4#6\", Parallel{typeof(vcat), Tuple{Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, typeof(flatten)}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Distributions.Categorical{P} where P<:Real, Random._GLOBAL_RNG}, Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularArrayBuffer{Float32, 3, Array{Float32, 3}}, CircularArrayBuffers.CircularArrayBuffer{Int64, 2, Matrix{Int64}}, CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}}}, env::MyGrid, stop_condition::StopAfterEpisode{ProgressMeter.Progress}, hook::TotalBatchRewardPerEpisode)",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/core/run.jl:10",
      " [13] top-level scope",
      "    @ In[5]:1",
      " [14] eval",
      "    @ ./boot.jl:373 [inlined]",
      " [15] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1196"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: both Losses and NNlib export \"ctc_loss\"; uses of it in module Flux must be qualified\n"
     ]
    }
   ],
   "source": [
    "run(agent, env, StopAfterEpisode(8), hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc2f856-3c35-4c3b-9baf-1cee79ecc6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc3cefd-0e43-4050-9481-6e83f6568fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7835c5-d1c7-4257-b6ba-f86c99f033f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
