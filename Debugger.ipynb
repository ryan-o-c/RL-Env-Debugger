{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6f2b1ac-50d6-42d7-960c-b970912a4abb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RL Import\n",
      "Preparing Environment\n",
      "Float32[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# MyGrid\n",
       "\n",
       "## Traits\n",
       "\n",
       "| Trait Type        |                          Value |\n",
       "|:----------------- | ------------------------------:|\n",
       "| NumAgentStyle     |                  SingleAgent() |\n",
       "| DynamicStyle      |                   Sequential() |\n",
       "| InformationStyle  |           PerfectInformation() |\n",
       "| ChanceStyle       |                Deterministic() |\n",
       "| RewardStyle       |                   StepReward() |\n",
       "| UtilityStyle      |                   GeneralSum() |\n",
       "| ActionStyle       |                FullActionSet() |\n",
       "| StateStyle        | Observation{Vector{Float32}}() |\n",
       "| DefaultStateStyle | Observation{Vector{Float32}}() |\n",
       "\n",
       "## Is Environment Terminated?\n",
       "\n",
       "No\n",
       "\n",
       "## State Space\n",
       "\n",
       "`Space{Vector{Interval{Int64, Closed, Closed}}}(Interval{Int64, Closed, Closed}[Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50)])`\n",
       "\n",
       "## Action Space\n",
       "\n",
       "`Base.OneTo(25)`\n",
       "\n",
       "## Current State\n",
       "\n",
       "```\n",
       "Float32[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "```\n"
      ],
      "text/plain": [
       "# MyGrid\n",
       "\n",
       "## Traits\n",
       "\n",
       "| Trait Type        |                          Value |\n",
       "|:----------------- | ------------------------------:|\n",
       "| NumAgentStyle     |                  SingleAgent() |\n",
       "| DynamicStyle      |                   Sequential() |\n",
       "| InformationStyle  |           PerfectInformation() |\n",
       "| ChanceStyle       |                Deterministic() |\n",
       "| RewardStyle       |                   StepReward() |\n",
       "| UtilityStyle      |                   GeneralSum() |\n",
       "| ActionStyle       |                FullActionSet() |\n",
       "| StateStyle        | Observation{Vector{Float32}}() |\n",
       "| DefaultStateStyle | Observation{Vector{Float32}}() |\n",
       "\n",
       "## Is Environment Terminated?\n",
       "\n",
       "No\n",
       "\n",
       "## State Space\n",
       "\n",
       "`Space{Vector{Interval{Int64, Closed, Closed}}}(Interval{Int64, Closed, Closed}[Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50), Interval{Int64, Closed, Closed}(0, 50)])`\n",
       "\n",
       "## Action Space\n",
       "\n",
       "`Base.OneTo(25)`\n",
       "\n",
       "## Current State\n",
       "\n",
       "```\n",
       "Float32[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "```\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ReinforcementLearning, Intervals, Flux\n",
    "\n",
    "const RLBase = ReinforcementLearningBase\n",
    "\n",
    "println(\"Done RL Import\")\n",
    "\n",
    "Base.@kwdef mutable struct MyGrid <: AbstractEnv\n",
    "    N::Int\n",
    "    obs::Vector{Float32}\n",
    "    max_steps::Int    \n",
    "    \n",
    "    rewards::Float32\n",
    "    done::Bool \n",
    "    t::Int\n",
    "end\n",
    "\n",
    "function MyGrid(; n)\n",
    "    img = zeros(Float32, n,n)\n",
    "    vect = ones(Float32, 3)\n",
    "    obs = vcat(vect,vec(img))\n",
    "    println(obs)\n",
    "    \n",
    "\tMyGrid(n, obs, 10, 0, false, 0)\n",
    "end\n",
    "\n",
    "RLBase.action_space(env::MyGrid) = Base.OneTo(25)\n",
    "\n",
    "RLBase.legal_action_space(env::MyGrid) = legal_action_space_mask(env)\n",
    "\n",
    "function RLBase.legal_action_space_mask(env::MyGrid)\n",
    "\tmask = Vector(undef, 25)\n",
    "\tfill!(mask, true)\n",
    "\n",
    "    mask_index = Base.rand((1:25))\n",
    "\tmask[mask_index] = false\n",
    "\treturn mask\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function (env::MyGrid)(action) #This function takes the action, makes the environment step, and gives reward\n",
    "    \n",
    "    env.obs[action[1]] = 6 #action[1] avoids some strange error I don't understand\n",
    "    \n",
    "    env.rewards+=1\n",
    "    \n",
    "    env.t+=1\n",
    "    #Max steps check\n",
    "    if env.t >= env.max_steps\n",
    "        env.done = true\n",
    "    end\n",
    "end\n",
    "\n",
    "RLBase.state(env::MyGrid, ::Observation{Vector{Float32}}) = env.obs\n",
    "RLBase.state_space(env::MyGrid, ::Observation{Vector{Float32}}) = Space(fill( Interval{Int64}(0, (10*env.N) ) , (env.N^2+3)))\n",
    "\n",
    "RLBase.is_terminated(env::MyGrid) = env.done\n",
    "\n",
    "function RLBase.reset!(env::MyGrid)\n",
    "    #Reset Counter and rewards (and termination variable)\n",
    "    env.t = 0\n",
    "    env.rewards = 0\n",
    "    env.done = false    \n",
    "    #Reset State\n",
    "    img = zeros(env.N,env.N)\n",
    "    vect = ones(3)\n",
    "    env.obs = vcat(vect,vec(img))\n",
    "end;\n",
    "\n",
    "RLBase.reward(env::MyGrid) = env.rewards\n",
    "\n",
    "RLBase.NumAgentStyle(::MyGrid) = SINGLE_AGENT\n",
    "RLBase.DynamicStyle(::MyGrid) = SEQUENTIAL\n",
    "RLBase.ActionStyle(::MyGrid) = FULL_ACTION_SET\n",
    "RLBase.InformationStyle(::MyGrid) = PERFECT_INFORMATION\n",
    "\n",
    "RLBase.StateStyle(::MyGrid) = Observation{Vector{Float32}}()\n",
    "\n",
    "RLBase.RewardStyle(::MyGrid) = STEP_REWARD\n",
    "RLBase.UtilityStyle(::MyGrid) = GENERAL_SUM\n",
    "RLBase.ChanceStyle(::MyGrid) = DETERMINISTIC\n",
    "\n",
    "println(\"Preparing Environment\")\n",
    "N=5\n",
    "env = MyGrid(; n=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99f8ee54-4350-40c1-bbbe-34645249b5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TotalBatchRewardPerEpisode([Float64[]], [0.0], true)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UPDATE_FREQ = 32\n",
    "N_ENV = 1\n",
    "input_size = 28\n",
    "\n",
    "agent = Agent(\n",
    "        policy = QBasedPolicy(\n",
    "            learner = A2CLearner(\n",
    "                approximator = ActorCritic(\n",
    "                    actor = Chain(\n",
    "                        Dense(input_size, 4*input_size, relu),\n",
    "                        Dense(4*input_size, 8*input_size, relu),\n",
    "                        Dense(8*input_size, 6*input_size, relu),\n",
    "                        Dense(6*input_size, 4*input_size, relu),\n",
    "                        Dense(4*input_size, 2*input_size, relu),\n",
    "                        Dense(2*input_size, 25),\n",
    "                    ),\n",
    "\n",
    "                    critic = Chain(\n",
    "                        Dense(input_size, 4*input_size, relu),\n",
    "                        Dense(4*input_size, 8*input_size, relu),\n",
    "                        Dense(8*input_size, 6*input_size, relu),\n",
    "                        Dense(6*input_size, 4*input_size, relu),\n",
    "                        Dense(4*input_size, 2*input_size, relu),\n",
    "                        Dense(2*input_size, 1),\n",
    "                    ),\n",
    "\n",
    "                    optimizer = ADAM(1e-3),\n",
    "                ),\n",
    "                γ = 0.99f0,\n",
    "                actor_loss_weight = 1.0f0,\n",
    "                critic_loss_weight = 0.5f0,\n",
    "                entropy_loss_weight = 0.001f0,\n",
    "                update_freq = UPDATE_FREQ,\n",
    "            ),\n",
    "\n",
    "            #explorer = BatchExplorer(GumbelSoftmaxExplorer()),\n",
    "            explorer=EpsilonGreedyExplorer(\n",
    "                kind=:exp,\n",
    "                ϵ_stable=0.01,\n",
    "                decay_steps=500,\n",
    "            ),\n",
    "        ),\n",
    "\n",
    "\n",
    "\t    trajectory = CircularArraySARTTrajectory(;\n",
    "            capacity = UPDATE_FREQ,\n",
    "            state = Matrix{Float32} => (input_size, N_ENV),\n",
    "            action = Vector{Int} => (N_ENV,),\n",
    "            reward = Vector{Float32} => (N_ENV,),\n",
    "            terminal = Vector{Bool} => (N_ENV,),\n",
    "        ),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "hook = TotalBatchRewardPerEpisode(N_ENV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ecb200d7-3196-4d75-aebe-27bada78b5e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: objects of type Vector{Float32} are not callable\nUse square brackets [] for indexing an Array.",
     "output_type": "error",
     "traceback": [
      "MethodError: objects of type Vector{Float32} are not callable\nUse square brackets [] for indexing an Array.",
      "",
      "Stacktrace:",
      "  [1] (::Base.var\"#260#261\"{Vector{Float32}})(::Pair{Int64, Any})",
      "    @ Base ./reduce.jl:803",
      "  [2] MappingRF",
      "    @ ./reduce.jl:95 [inlined]",
      "  [3] _foldl_impl",
      "    @ ./reduce.jl:58 [inlined]",
      "  [4] foldl_impl(op::Base.MappingRF{Base.var\"#260#261\"{Vector{Float32}}, Base.BottomRF{typeof(Base._rf_findmax)}}, nt::Base._InitialValue, itr::Base.Pairs{Int64, Any, LinearIndices{1, Tuple{Base.OneTo{Int64}}}, Vector{Any}})",
      "    @ Base ./reduce.jl:48",
      "  [5] mapfoldl_impl(f::Base.var\"#260#261\"{Vector{Float32}}, op::typeof(Base._rf_findmax), nt::Base._InitialValue, itr::Base.Pairs{Int64, Any, LinearIndices{1, Tuple{Base.OneTo{Int64}}}, Vector{Any}})",
      "    @ Base ./reduce.jl:44",
      "  [6] mapfoldl(f::Function, op::Function, itr::Base.Pairs{Int64, Any, LinearIndices{1, Tuple{Base.OneTo{Int64}}}, Vector{Any}}; init::Base._InitialValue)",
      "    @ Base ./reduce.jl:162",
      "  [7] mapfoldl(f::Function, op::Function, itr::Base.Pairs{Int64, Any, LinearIndices{1, Tuple{Base.OneTo{Int64}}}, Vector{Any}})",
      "    @ Base ./reduce.jl:162",
      "  [8] findmax(f::Vector{Float32}, domain::Vector{Any})",
      "    @ Base ./reduce.jl:803",
      "  [9] (::EpsilonGreedyExplorer{:exp, false, Random._GLOBAL_RNG})(values::Vector{Float32}, mask::Vector{Any})",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/policies/q_based_policies/explorers/epsilon_greedy_explorer.jl:128",
      " [10] (::QBasedPolicy{A2CLearner{ActorCritic{Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}}, EpsilonGreedyExplorer{:exp, false, Random._GLOBAL_RNG}})(env::MyGrid, #unused#::FullActionSet, #unused#::Base.OneTo{Int64})",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/policies/q_based_policies/q_based_policy.jl:25",
      " [11] QBasedPolicy",
      "    @ ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/policies/q_based_policies/q_based_policy.jl:22 [inlined]",
      " [12] Agent",
      "    @ ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/policies/agents/agent.jl:24 [inlined]",
      " [13] _run(policy::Agent{QBasedPolicy{A2CLearner{ActorCritic{Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}}, EpsilonGreedyExplorer{:exp, false, Random._GLOBAL_RNG}}, CircularArraySARTTrajectory{NamedTuple{(:state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32, 3, Array{Float32, 3}}, CircularArrayBuffers.CircularArrayBuffer{Int64, 2, Matrix{Int64}}, CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}}}, env::MyGrid, stop_condition::StopAfterEpisode{ProgressMeter.Progress}, hook::TotalBatchRewardPerEpisode)",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/core/run.jl:27",
      " [14] run(policy::Agent{QBasedPolicy{A2CLearner{ActorCritic{Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}}, EpsilonGreedyExplorer{:exp, false, Random._GLOBAL_RNG}}, CircularArraySARTTrajectory{NamedTuple{(:state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32, 3, Array{Float32, 3}}, CircularArrayBuffers.CircularArrayBuffer{Int64, 2, Matrix{Int64}}, CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}}}, env::MyGrid, stop_condition::StopAfterEpisode{ProgressMeter.Progress}, hook::TotalBatchRewardPerEpisode)",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/core/run.jl:10",
      " [15] top-level scope",
      "    @ In[38]:1",
      " [16] eval",
      "    @ ./boot.jl:373 [inlined]",
      " [17] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "run(agent, env, StopAfterEpisode(8), hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c630bd5e-8669-46d3-b675-c97b5697b4e0",
   "metadata": {},
   "source": [
    "Old Code for complex state version of environment (Ignore!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b7c0a9f-6e74-4076-a31f-9cf576940d07",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TotalBatchRewardPerEpisode([Float64[]], [0.0], true)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UPDATE_FREQ = 32\n",
    "N_ENV = 1\n",
    "\n",
    "agent = Agent(\n",
    "        policy = PPOPolicy(\n",
    "            approximator = ActorCritic(\n",
    "                    actor = Chain(\n",
    "                                x -> ( reshape(x[4:28], (5,5,1,1)), x[1:3]), #Parallel expects a tuple input - very difficult to pass a tuple as state\n",
    "                                Parallel(vcat, \n",
    "                                        Chain(Conv((3,3), 1 => 1, relu; stride = 1, pad = 0),\n",
    "                                        Flux.flatten),\n",
    "\n",
    "                                        Chain(Dense(3, 6, relu), Dense(6, 3, relu), Dense(3, 1, relu)) ),\n",
    "                                Dense(10, 40, relu),\n",
    "                                Dense(40,80,relu),\n",
    "                                Dense(80,50,relu),\n",
    "                                Dense(50,25)),\n",
    "\n",
    "                    critic = Chain(\n",
    "                                x -> ( reshape(x[4:28], (5,5,1,1)), x[1:3]),\n",
    "                                Parallel(vcat, \n",
    "                                        Chain(Conv((3,3), 1 => 1, relu; stride = 1, pad = 0),\n",
    "                                        Flux.flatten),\n",
    "\n",
    "                                        Chain(Dense(3, 6, relu), Dense(6, 3, relu), Dense(3, 1, relu)) ),\n",
    "                                Dense(10, 40, relu),\n",
    "                                Dense(40,80,relu),\n",
    "                                Dense(80,50,relu),\n",
    "                                Dense(50,25,relu),\n",
    "                                Dense(25,5,relu),\n",
    "                                Dense(5,1)),\n",
    "            \n",
    "                optimizer = ADAM(1e-3),\n",
    "            ),\n",
    "            γ = 0.99f0,\n",
    "            λ = 0.95f0,\n",
    "            clip_range = 0.1f0,\n",
    "            max_grad_norm = 0.5f0,\n",
    "            n_epochs = 4,\n",
    "            n_microbatches = 4,\n",
    "            actor_loss_weight = 1.0f0,\n",
    "            critic_loss_weight = 0.5f0,\n",
    "            entropy_loss_weight = 0.001f0,\n",
    "            update_freq = UPDATE_FREQ,\n",
    "        ),\n",
    "        trajectory = PPOTrajectory(;\n",
    "            capacity = UPDATE_FREQ, \n",
    "            state = Matrix{Float32} => (28, N_ENV),\n",
    "            action = Vector{Int} => (N_ENV,),\n",
    "            action_log_prob = Vector{Float32} => (N_ENV,),\n",
    "            reward = Vector{Float32} => (N_ENV,),  \n",
    "            terminal = Vector{Bool} => (N_ENV,),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "#stop_condition = StopAfterStep(30000, is_show_progress=true)\n",
    "hook = TotalBatchRewardPerEpisode(N_ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45a24388-252d-4358-b108-3e783c343514",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  25%|██████████▎                              |  ETA: 0:00:03\u001b[39m"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "BoundsError: attempt to access 1-element view(::Matrix{Float32}, 1, :) with eltype Float32 at index [33]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 1-element view(::Matrix{Float32}, 1, :) with eltype Float32 at index [33]",
      "",
      "Stacktrace:",
      "  [1] throw_boundserror(A::SubArray{Float32, 1, Matrix{Float32}, Tuple{Int64, Base.Slice{Base.OneTo{Int64}}}, true}, I::Tuple{Int64})",
      "    @ Base ./abstractarray.jl:691",
      "  [2] checkbounds",
      "    @ ./abstractarray.jl:656 [inlined]",
      "  [3] getindex",
      "    @ ./subarray.jl:302 [inlined]",
      "  [4] _generalized_advantage_estimation!(advantages::SubArray{Float32, 1, Matrix{Float32}, Tuple{Int64, Base.Slice{Base.OneTo{Int64}}}, true}, rewards::SubArray{Float32, 1, Matrix{Float32}, Tuple{Int64, Vector{Int64}}, false}, values::SubArray{Float32, 1, Matrix{Float32}, Tuple{Int64, Base.Slice{Base.OneTo{Int64}}}, true}, γ::Float32, λ::Float32, terminal::SubArray{Bool, 1, Matrix{Bool}, Tuple{Int64, Vector{Int64}}, false})",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/utils/base.jl:445",
      "  [5] _generalized_advantage_estimation!(advantages::Matrix{Float32}, rewards::CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, values::Matrix{Float32}, γ::Float32, λ::Float32, terminal::CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}, dims::Int64)",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/utils/base.jl:425",
      "  [6] #generalized_advantage_estimation!#24",
      "    @ ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/utils/base.jl:387 [inlined]",
      "  [7] #generalized_advantage_estimation#23",
      "    @ ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/utils/base.jl:375 [inlined]",
      "  [8] _update!(p::PPOPolicy{ActorCritic{Chain{Tuple{var\"#9#11\", Parallel{typeof(vcat), Tuple{Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, typeof(flatten)}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{var\"#10#12\", Parallel{typeof(vcat), Tuple{Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, typeof(flatten)}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Distributions.Categorical{P} where P<:Real, Random._GLOBAL_RNG}, t::Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularArrayBuffer{Float32, 3, Array{Float32, 3}}, CircularArrayBuffers.CircularArrayBuffer{Int64, 2, Matrix{Int64}}, CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}})",
      "    @ ReinforcementLearningZoo ~/.julia/packages/ReinforcementLearningZoo/uNyVA/src/algorithms/policy_gradient/ppo.jl:244",
      "  [9] update!",
      "    @ ~/.julia/packages/ReinforcementLearningZoo/uNyVA/src/algorithms/policy_gradient/ppo.jl:210 [inlined]",
      " [10] (::Agent{PPOPolicy{ActorCritic{Chain{Tuple{var\"#9#11\", Parallel{typeof(vcat), Tuple{Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, typeof(flatten)}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{var\"#10#12\", Parallel{typeof(vcat), Tuple{Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, typeof(flatten)}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Distributions.Categorical{P} where P<:Real, Random._GLOBAL_RNG}, Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularArrayBuffer{Float32, 3, Array{Float32, 3}}, CircularArrayBuffers.CircularArrayBuffer{Int64, 2, Matrix{Int64}}, CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}}})(stage::PreActStage, env::MyGrid, action::Vector{Int64})",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/policies/agents/agent.jl:78",
      " [11] _run(policy::Agent{PPOPolicy{ActorCritic{Chain{Tuple{var\"#9#11\", Parallel{typeof(vcat), Tuple{Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, typeof(flatten)}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{var\"#10#12\", Parallel{typeof(vcat), Tuple{Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, typeof(flatten)}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Distributions.Categorical{P} where P<:Real, Random._GLOBAL_RNG}, Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularArrayBuffer{Float32, 3, Array{Float32, 3}}, CircularArrayBuffers.CircularArrayBuffer{Int64, 2, Matrix{Int64}}, CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}}}, env::MyGrid, stop_condition::StopAfterEpisode{ProgressMeter.Progress}, hook::TotalBatchRewardPerEpisode)",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/core/run.jl:29",
      " [12] run(policy::Agent{PPOPolicy{ActorCritic{Chain{Tuple{var\"#9#11\", Parallel{typeof(vcat), Tuple{Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, typeof(flatten)}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{var\"#10#12\", Parallel{typeof(vcat), Tuple{Chain{Tuple{Conv{2, 4, typeof(relu), Array{Float32, 4}, Vector{Float32}}, typeof(flatten)}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Distributions.Categorical{P} where P<:Real, Random._GLOBAL_RNG}, Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularArrayBuffer{Float32, 3, Array{Float32, 3}}, CircularArrayBuffers.CircularArrayBuffer{Int64, 2, Matrix{Int64}}, CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}}}, env::MyGrid, stop_condition::StopAfterEpisode{ProgressMeter.Progress}, hook::TotalBatchRewardPerEpisode)",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/s9XPF/src/core/run.jl:10",
      " [13] top-level scope",
      "    @ In[36]:1",
      " [14] eval",
      "    @ ./boot.jl:373 [inlined]",
      " [15] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "run(agent, env, StopAfterEpisode(8), hook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
